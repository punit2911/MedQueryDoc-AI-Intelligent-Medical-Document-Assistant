# ğŸ©º Medical Document Intelligence Chatbot (LLaMA-2)

## ğŸ“Œ About the Project

This project is an AI-powered **medical document question-answering system** built using **Large Language Models (LLMs)** and **vector-based information retrieval**. It enables users to ask natural language questions and receive accurate, context-aware answers derived from medical PDF documents.

The system works by loading and splitting PDF documents into smaller chunks, generating vector embeddings using a Hugging Face embedding model, and storing them in **Pinecone**, a scalable vector database. During inference, user queries are matched against relevant document chunks using similarity search, and responses are generated using **LLaMA-2** through a retrieval-augmented generation (RAG) pipeline.

This project demonstrates practical implementation of **LLMs + LangChain + Vector Databases** for real-world AI applications.

---

## ğŸ§  Key Features

* PDF-based medical document ingestion
* Vector similarity search using Pinecone
* Retrieval-Augmented Generation (RAG) pipeline
* Local LLaMA-2 model inference
* Interactive web interface using Flask
* Modular and extensible architecture

---

## ğŸ› ï¸ Built With

* **Python**
* **LangChain**
* **Flask**
* **Meta LLaMA-2**
* **Pinecone (Vector Database)**
* **Hugging Face Embeddings**

---

## ğŸš€ Getting Started

Follow the steps below to set up and run the project locally.

---

## âš™ï¸ Installation Steps

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/Medical-Chatbot-using-Llama-2.git
cd Medical-Chatbot-using-Llama-2
```

---

### 2ï¸âƒ£ Create a Virtual Environment (Recommended)

> If Conda is installed:

```bash
conda create -p ./venv python=3.10 -y
conda activate ./venv
```

> Or using Python venv:

```bash
python -m venv venv
venv\Scripts\activate   # Windows
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” API Key Setup

Create a `.env` file in the root directory and add your Pinecone credentials:

```ini
PINECONE_API_KEY=your_pinecone_api_key_here
```

> âš ï¸ Note: Pinecone no longer requires an environment variable (`PINECONE_API_ENV`) in the new SDK.

---

## ğŸ“„ Add Medical Documents

Create a `data/` folder in the project root and add your PDF files:

```
data/
â”œâ”€â”€ medical_doc_1.pdf
â”œâ”€â”€ medical_doc_2.pdf
```

---

## ğŸ§¬ Index Documents into Pinecone

Before running the chatbot, index the documents:

```bash
python store_index.py
```

This step:

* Loads PDFs
* Splits text into chunks
* Generates embeddings
* Stores vectors in Pinecone

---

## â–¶ï¸ Run the Application

```bash
python app.py
```

Then open your browser and visit:

```
http://localhost:8080
```

---

## ğŸ—ï¸ Project Architecture

```
â”œâ”€â”€ app.py                 # Flask application
â”œâ”€â”€ store_index.py         # PDF ingestion & vector indexing
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py          # PDF loading, chunking, embeddings
â”‚   â”œâ”€â”€ prompt.py          # Prompt template
â”œâ”€â”€ model/
â”‚   â””â”€â”€ llama-2-7b-chat.ggmlv3.q4_0.bin
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css
â”œâ”€â”€ data/                  # PDF documents
â””â”€â”€ .env
```

---

## ğŸ§  Model Setup

Download the quantized LLaMA-2 model and place it inside the `model/` directory:

```text
Model: llama-2-7b-chat.ggmlv3.q4_0.bin
Source:
https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML
```

---

## ğŸš§ Future Enhancements

* Multilingual medical support
* Source citation in answers
* Document upload via UI
* Streaming responses
* Cloud-hosted inference
* Authentication & access control

---

## ğŸ“œ License

This project is licensed under the **GNU General Public License v3.0**.
See the `LICENSE` file for more details.

---

## â­ Acknowledgements

This project builds upon open-source tools and research in the fields of **LLMs**, **NLP**, and **vector databases**.
Special thanks to the open-source community for enabling practical AI innovation.

---


